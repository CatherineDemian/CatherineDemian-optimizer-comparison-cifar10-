# CatherineDemian-optimizer-comparison-cifar10-

 Optimization Algorithm Comparison Using CIFAR-10

This project evaluates the performance of several optimization algorithms for training a shallow neural network on the CIFAR-10 dataset.

---

## ðŸ§ª Optimization Algorithms Compared

1. **Stochastic Gradient Descent with Warm Restarts (SGDR)**
2. **Nesterov Accelerated Gradient (NAG)**
3. **RMSProp**
4. **Nadam**
5. **Learning Rate Schedulers**
   - Exponential Decay
   - Step Decay

---

## ðŸ§  Model

- A shallow neural network (e.g., 1â€“2 hidden layers)
- Activation: ReLU
- Output: Softmax for classification
- Dataset: **CIFAR-10** (10-class image classification dataset)
